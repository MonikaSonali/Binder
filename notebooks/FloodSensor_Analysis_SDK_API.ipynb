{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9dc12e6",
   "metadata": {},
   "source": [
    "# Summary  :\n",
    "\n",
    "           -> 46 Sensors analysed \n",
    "           \n",
    "           -> Data Time Range : 2022-01-10 to 2022-05-02\n",
    "           \n",
    "           -> FWR022, FWR033, FWR049 have not given any data in this time range\n",
    "           \n",
    "           -> FWR060 -> No Data from 2022-04-01\n",
    "           \n",
    "           -> FWR046, FWR039, FWR035, FWR021, FWR037 have all-zero measures accounting to \n",
    "              94%, 92%, 84%, 32%, 24% of their data respectively\n",
    "           \n",
    "           \n",
    "           -> FWR035 -> Data between 2022-04-20 to 2022-04-27 only with 84% of data being all-zero measures                 (currentLevel, measuredDistance, referenceLevel )          \n",
    "           \n",
    "           -> All sensors' data have around 0.05% - 0.1% of duplicates only\n",
    "            \n",
    "           -> 'CurrentLevel = ReferenceLevel - MeasuredDistance' holds good for all sensors except FWR021.\n",
    "             For FWR021, around 51 packets deviate from this criteria (differ by a constant value of 0.5).\n",
    "             \n",
    "       \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d894d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5de85d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a8e6ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Sensors that have given data :  0 out of 46 total sensors\n",
      "Flood Sensors which have not given data are :  ['FWR055', 'FWR018', 'FWR037', 'FWR054', 'FWR036', 'FWR065', 'FWR045', 'FWR024', 'FWR050', 'FWR067', 'FWR013', 'FWR025', 'FWR035', 'FWR064', 'FWR017', 'FWR070', 'FWR021', 'FWR022', 'FWR060', 'FWR020', 'FWR033', 'FWR059', 'FWR019', 'FWR056', 'FWR069', 'FWR016', 'FWR051', 'FWR030', 'FWR047', 'FWR046', 'FWR026', 'FWR015', 'FWR044', 'FWR039', 'FWR052', 'FWR063', 'FWR048', 'FWR066', 'FWR023', 'FWR034', 'FWR068', 'FWR058', 'FWR057', 'FWR061', 'FWR049', 'FWR053']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "csvfiles = []\n",
    "for file in glob.glob(\"*.csv\"):\n",
    "    csvfiles.append(file)\n",
    "\n",
    "print(\"No of Sensors that have given data : \",len(csvfiles), \"out of 46 total sensors\")\n",
    "\n",
    "# csvfiles\n",
    "\n",
    "L = ['FWR013','FWR015','FWR016','FWR017','FWR018','FWR019','FWR020','FWR021','FWR022','FWR023','FWR024',\n",
    " 'FWR025','FWR026','FWR030','FWR033','FWR034','FWR035','FWR036','FWR037','FWR039','FWR044','FWR045',\n",
    " 'FWR046','FWR047','FWR048','FWR049','FWR050','FWR051','FWR052','FWR053','FWR054','FWR055','FWR056',\n",
    " 'FWR057','FWR058','FWR059','FWR060','FWR061','FWR063','FWR064','FWR065','FWR066','FWR067','FWR068',\n",
    " 'FWR069','FWR070']\n",
    "Allsensors = [ \"pune_flood_\"+i+\".csv\" for i in L]\n",
    "\n",
    "# \n",
    "# print (\"Allsensors\",Allsensors)\n",
    "# print()\n",
    "# print (\"csvfiles\",csvfiles)\n",
    "# print()\n",
    "# print([ i.split(\"pune_flood_\")[1].split(\".\")[0] for i in set(Allsensors)-set(csvfiles)])\n",
    "print(\"Flood Sensors which have not given data are : \", [ i.split(\"pune_flood_\")[1].split(\".\")[0] for i in set(Allsensors)-set(csvfiles)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87f2f0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkDuplicates(df):\n",
    "    d = {}\n",
    "    d[\"Total Count\"] = df.shape[0]\n",
    "    d[\"Duplicate Count\"] = df[df.duplicated()].shape[0]\n",
    "    d[\" Duplicate Percentage\"] = round((df[df.duplicated()].shape[0]/df.shape[0])*100,2)\n",
    "    if df[df.duplicated()].shape[0] != 0: \n",
    "        return \"Duplicates Present ... |\"+ str(d)\n",
    "    else:\n",
    "        return \"No Duplicated Found |\"+ str(d)\n",
    "    \n",
    "    \n",
    "def dataprocessing(df):\n",
    "    df.drop([\"id\"],axis=1,inplace=True)\n",
    "    df[\"observationDateTime\"]=pd.to_datetime(df[\"observationDateTime\"],errors ='coerce')\n",
    "    df[\"date\"] = df[\"observationDateTime\"].dt.date\n",
    "    df['date'] = pd.to_datetime(df['date'],format=\"%Y-%m-%d\")\n",
    "    df[\"checkCurrentLevel\"] = round((df[\"referenceLevel\"]-df[\"measuredDistance\"]),2)-df[\"currentLevel\"]\n",
    "#     print(\"checkCurrentLevel value \",df[\"checkCurrentLevel\"].unique())\n",
    "    return df\n",
    "\n",
    "def plot(df):\n",
    "    f,ax=plt.subplots(3,2, figsize=(20,10))\n",
    "    sns.lineplot(x=\"observationDateTime\",y=\"currentLevel\",data = df, ax=ax[0,0])\n",
    "    sns.lineplot(x=\"observationDateTime\",y=\"measuredDistance\",data = df, ax=ax[0,1])\n",
    "    sns.lineplot(x=\"observationDateTime\",y=\"referenceLevel\",data = df, ax=ax[1,0])\n",
    "    sns.lineplot(x=\"referenceLevel\",y=\"currentLevel\",data = df, ax=ax[1,1])\n",
    "    sns.lineplot(x=\"currentLevel\",y=\"measuredDistance\",data = df, ax=ax[2,0])\n",
    "    sns.lineplot(x=\"referenceLevel\",y=\"measuredDistance\",data = df, ax=ax[2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5d7a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "for file in csvfiles:\n",
    "    d = {}\n",
    "    df = pd.read_csv(file)\n",
    "    df[\"Sensor\"] = file[11:17]\n",
    "#     print(file[11:17])\n",
    "#     print(checkDuplicates(df))\n",
    "    \n",
    "    dup = checkDuplicates(df)\n",
    "    df = dataprocessing(df)\n",
    "#     col = \"currentLevel\"\n",
    "#     df[col].value_counts()\n",
    "#     df[col].unique()\n",
    "    j = ast.literal_eval(dup.split(\"|\")[1])\n",
    "    d[\"Sensor\"] = df[\"Sensor\"][0]\n",
    "    d[\"MaxMeasuredDistance\"] = df[\"measuredDistance\"].max()\n",
    "    d[\"TotalCount\"] = j[\"Total Count\"]\n",
    "    d[\"Duplicate Count\"] = j[\"Duplicate Count\"]\n",
    "    d[\"Duplicate Percentage\"] = j[\" Duplicate Percentage\"]\n",
    "    d[\"CheckCurrentLevel\"] = df[\"checkCurrentLevel\"].unique()\n",
    "    d[\"AllZeroCount\"] = df[(df[\"currentLevel\"]==0.0) & (df[\"measuredDistance\"]==0.0) & (df[\"referenceLevel\"]==0.0)].shape[0] \n",
    "    d[\"AllZeroPercentage\"] = round((d[\"AllZeroCount\"]/d[\"TotalCount\"])*100,2)\n",
    "    L.append(d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "111dded0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TotalCount'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m DF \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(L)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mDF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTotalCount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m DF \u001b[38;5;241m=\u001b[39m DF\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAllZeroPercentage\u001b[39m\u001b[38;5;124m\"\u001b[39m], ascending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:6313\u001b[0m, in \u001b[0;36mDataFrame.sort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   6309\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(by):\n\u001b[1;32m   6310\u001b[0m     \u001b[38;5;66;03m# len(by) == 1\u001b[39;00m\n\u001b[1;32m   6312\u001b[0m     by \u001b[38;5;241m=\u001b[39m by[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 6313\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label_or_level_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6315\u001b[0m     \u001b[38;5;66;03m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[1;32m   6316\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6317\u001b[0m         \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[1;32m   6318\u001b[0m         \u001b[38;5;66;03m# \"Series\", variable has type \"ndarray\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:1840\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1838\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mget_level_values(key)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m   1842\u001b[0m \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TotalCount'"
     ]
    }
   ],
   "source": [
    "DF = pd.DataFrame(L)\n",
    "DF.sort_values(by=[\"TotalCount\"], ascending = False)\n",
    "DF = DF.sort_values(by=[\"AllZeroPercentage\"], ascending = False)\n",
    "# print(\" ----- \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd6761d",
   "metadata": {},
   "source": [
    "# Percentage of duplicate data present - for all sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafe4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(DF, values=\"Duplicate Percentage\", names= str(\"Sensor\"), height=500, hole=0.5)\n",
    "fig.update_traces(textposition='inside', textinfo='percent')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b377ffe",
   "metadata": {},
   "source": [
    "# Sensor wise depiction of percentage of all-zero measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea03ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(DF, values=\"AllZeroPercentage\", names= str(\"Sensor\"), height=500, hole=0.5)\n",
    "fig.update_traces(textposition='inside', textinfo='percent')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd821de7",
   "metadata": {},
   "source": [
    "# Sensor Outage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"pune_flood_FWR023.csv\")\n",
    "df[\"observationDateTime\"]=pd.to_datetime(df[\"observationDateTime\"],errors ='coerce')\n",
    "df.sort_values(by = [\"observationDateTime\"], inplace=True)\n",
    "# df.info()\n",
    "df.reset_index(inplace=True)\n",
    "df.drop([\"index\"],axis=1, inplace=True)\n",
    "df.head()\n",
    "\n",
    "df[\"publishInterval\"] = pd.to_timedelta('0 days 00:00:00.0000')\n",
    "df[\"toDateTime\"] = \"\"\n",
    "for i,row in df.iterrows():\n",
    "    if i != df.shape[0]-1:\n",
    "#         df[\"publishInterval\"][i] = df[\"observationDateTime\"][i+1] - df[\"observationDateTime\"][i]\n",
    "        df[\"toDateTime\"][i] = df[\"observationDateTime\"][i+1]\n",
    "        df[\"publishInterval\"][i] = df[\"toDateTime\"][i] - df[\"observationDateTime\"][i]\n",
    "    \n",
    "    \n",
    "df[\"seconds\"] = df[\"publishInterval\"].apply(lambda x: x.total_seconds())\n",
    "df[\"PI (in minutes)\"] = df[\"seconds\"]/60\n",
    "df = df.sort_values(by = [\"observationDateTime\"], ascending = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131cea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df, x=df.observationDateTime, y = df[\"PI (in minutes)\"], hover_data=[df.observationDateTime, df.toDateTime, df.publishInterval])\n",
    "fig.update_layout(template=\"plotly_dark\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a1ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index([\"observationDateTime\"],inplace = True)\n",
    "# display(df)\n",
    "# df.reset_index(inplace=True)\n",
    "# df.drop([\"index\"],axis=1, inplace=True)\n",
    "# # df[[\"observationDateTime\",\"toDateTime\",\"publishInterval\"]]\n",
    "df1 = df.resample('W')[[\"toDateTime\",\"seconds\"]].mean()\n",
    "df1[\"mins\"] = df1[\"seconds\"]/60\n",
    "df1.reset_index(inplace=True)\n",
    "\n",
    "startInd = df1[df1[\"mins\"] == df1[\"mins\"].max()].index.values\n",
    "endInd = df1[df1[\"mins\"] == df1[\"mins\"].max()].index.values + 1 \n",
    "# print(startInd,endInd)\n",
    "# print(df1.iloc[startInd,0].values, df1.iloc[endInd,0].values)\n",
    "print(\"MAXIMUM WEEKLY AVG PUBLISH INTERVAL(in mins) is \", df1[\"mins\"].max(), \"between the dates \",df1.iloc[startInd,0].values,\" and \",endInd)\n",
    "df1.rename(columns = {'observationDateTime':'WeekStartDateTime', 'seconds':'AvgSeconds', 'mins':'AvgMins'}, inplace = True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5a27e",
   "metadata": {},
   "source": [
    "# Maximum measured distance by each flood sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef91512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(figsize(20,30))\n",
    "plt.figure(figsize=[15,7])\n",
    "sns.lineplot(x=\"Sensor\", y= \"MaxMeasuredDistance\", data=DF)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"pune_flood_FWR060.csv\")\n",
    "# df.sort_values(by=[\"observationDateTime\"])\n",
    "# df\n",
    "# df[(df[\"currentLevel\"]==0.0) & (df[\"measuredDistance\"]==0.0) & (df[\"referenceLevel\"]==0.0)]\n",
    "\n",
    "# df[(df[\"currentLevel\"]==0.0) & (df[\"measuredDistance\"]==0.0) & (df[\"referenceLevel\"]==0.0)].shape[0] \n",
    "\n",
    "\n",
    "# FWR046, FWR039, FWR035, FWR021, FWR037"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da982a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f,ax=plt.subplots(3,2, figsize=(20,10))\n",
    "# sns.lineplot(x=\"observationDateTime\",y=\"currentLevel\",data = df, ax=ax[0,0])\n",
    "# plt.xticks(rotation = 30)\n",
    "# sns.lineplot(x=\"observationDateTime\",y=\"measuredDistance\",data = df, ax=ax[0,1])\n",
    "# sns.lineplot(x=\"observationDateTime\",y=\"referenceLevel\",data = df, ax=ax[1,0])\n",
    "# sns.lineplot(x=\"referenceLevel\",y=\"currentLevel\",data = df, ax=ax[1,1])\n",
    "# sns.lineplot(x=\"currentLevel\",y=\"measuredDistance\",data = df, ax=ax[2,0])\n",
    "# sns.lineplot(x=\"referenceLevel\",y=\"measuredDistance\",data = df, ax=ax[2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454734e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f,ax=plt.subplots(3,2, figsize=(20,10))\n",
    "# sns.lineplot(x=\"observationDateTime\",y=\"currentLevel\",data = df3, ax=ax[0,0])\n",
    "# plt.xticks(rotation = 30)\n",
    "# sns.lineplot(x=\"observationDateTime\",y=\"measuredDistance\",data = df3, ax=ax[0,1])\n",
    "# sns.lineplot(x=\"observationDateTime\",y=\"referenceLevel\",data = df3, ax=ax[1,0])\n",
    "# sns.lineplot(x=\"referenceLevel\",y=\"currentLevel\",data = df3, ax=ax[1,1])\n",
    "# sns.lineplot(x=\"currentLevel\",y=\"measuredDistance\",data = df3, ax=ax[2,0])\n",
    "# sns.lineplot(x=\"referenceLevel\",y=\"measuredDistance\",data = df3, ax=ax[2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e472d283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d79ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from iudx.rs.ResourceServer import ResourceServer\n",
    "# from iudx.rs.ResourceQuery import ResourceQuery\n",
    "\n",
    "# # entity id for the pune env aqm sensor.\n",
    "# entity_id = \"datakaveri.org/04a15c9960ffda227e9546f3f46e629e1fe4132b/rs.iudx.org.in/pune-env-flood/FWR065\"\n",
    "\n",
    "# # creating an object of ResourceServer class using rs_url.\n",
    "# rs = ResourceServer(\n",
    "#          rs_url=\"https://rs.iudx.org.in/ngsi-ld/v1\",\n",
    "#          headers={\"content-type\": \"application/json\",\n",
    "#                   \"token\":\"eyJ0eXAiOiJKV1QiLCJhbGciOiJFUzI1NiJ9.eyJzdWIiOiIyOWVjNGZjOS03NDVhLTQyNjEtOTA3My02YTI5OGE3ZjQwZGUiLCJpc3MiOiJhdXRob3JpemF0aW9uLml1ZHgub3JnLmluIiwiYXVkIjoicnMuaXVkeC5vcmcuaW4iLCJleHAiOjE2NTEyNTY2MzEsImlhdCI6MTY1MTIxMzQzMSwiaWlkIjoicnM6cnMuaXVkeC5vcmcuaW4iLCJyb2xlIjoiY29uc3VtZXIiLCJjb25zIjp7fX0.T4QKwcBSwwuZ_neEiXS56TiAR-Q6G4wnQwbxNq8M9u9s99ylIY_qI0yw-utz7izEVcGqn0lDOZIsPU_Bog3jZw\"\n",
    "#                  }\n",
    "#      )\n",
    "\n",
    "# # creating a query for fetching latest data for the entity_id.\n",
    "# rs_query = ResourceQuery()\n",
    "# rs_entity = rs_query.add_entity(entity_id)\n",
    "\n",
    "# # fetch results for a list of entities.\n",
    "# results = rs.get_latest([rs_entity])\n",
    "\n",
    "# # printing results\n",
    "# print(f\"RESULTS: {results[0].results}\")        # get the result data of the resource query.\n",
    "# print(f\"STATUS: {results[0].type}\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51510d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df4[df4[\"currentLevel\"]==0.0])\n",
    "# print(df4[df4[\"currentLevel\"]==0.0].shape[0]/df4.shape[0])\n",
    "# df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b43ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8de09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aedf86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf3fc99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a2f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d70a16c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d839f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b3ec12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
